{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5242c0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! kill -9 $(nvidia-smi --query-compute-apps=pid --format=csv,noheader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81e7848f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr 24 08:19:57 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  | 00000000:47:00.0 Off |                    0 |\n",
      "| N/A   27C    P0              57W / 400W |      0MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4a60779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-24 08:20:46 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 08:20:52.066617: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745497252.495211  263942 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745497252.596952  263942 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745497253.689265  263942 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745497253.689295  263942 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745497253.689297  263942 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745497253.689299  263942 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-24 08:20:53.799016: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "from typing import Optional, Union\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2304cbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "finetune = \"/network/rit/lab/wang_lab_cs/ptian/output/rft/qwen2.5-1.5b-instruct-grpo-20250423-080507\"\n",
    "# finetune = \"grpo_finetuned_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4aa1d44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FewShotEvaluator:\n",
    "    \"\"\"\n",
    "    Few-shot evaluator for math reasoning tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset: Dataset, n_shots: int = 3, device: str = \"cuda\", batch_size: int = 16) -> None:\n",
    "        self.dataset = dataset\n",
    "        self.n_shots = n_shots\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.fewshot_prompt = self.get_fewshot_prompt()\n",
    "\n",
    "    def get_fewshot_prompt(self) -> str:\n",
    "        prompt = \"Solve these math problems:\\n\\n\"\n",
    "        for i in range(self.n_shots):\n",
    "            example = self.dataset[i]\n",
    "            prompt += f\"Question: {example['question']}\\nAnswer: {example['answer']}\" + \"\\n\\n\"\n",
    "        return prompt\n",
    "\n",
    "    def preprocess_eval(self, examples: dict) -> dict:\n",
    "        # Preprocess the example to include the few-shot prompt\n",
    "        return {\n",
    "            \"prompt\": [self.fewshot_prompt + f\"Question: {question}\\nAnswer:\\n\" for question in examples[\"question\"]]\n",
    "        }\n",
    "\n",
    "    def parse_answer(self, answer: str) -> Optional[str]:\n",
    "        # Extract the answer from the generated text\n",
    "        try:\n",
    "            predicted_answer = re.search(r\"#### (-?\\d+\\.?\\d*)\", answer).group(1)\n",
    "        except:\n",
    "            predicted_answer = None\n",
    "        return predicted_answer\n",
    "\n",
    "    # def eval(self, model_path: str, tokenizer: AutoTokenizer, device: str = \"cuda\", temperature: float = 0.7, top_p: float = 0.95, max_tokens: int = 256) -> float:\n",
    "    def eval(self, model_path: str, dtype: str = \"auto\", device: str = \"cuda\", temperature: float = 0.7, top_p: float = 0.95, max_tokens: int = 256) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate exact match accuracy\n",
    "        \"\"\"\n",
    "        # Load dataset\n",
    "        eval_dataset = self.dataset.select(range(self.n_shots, len(self.dataset)))\n",
    "        eval_dataset = eval_dataset.map(self.preprocess_eval, batched=True)\n",
    "        eval_dataloader = DataLoader(eval_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        # Load model\n",
    "        llm = LLM(model=model_path, dtype=dtype)\n",
    "        # Shared or individual sampling settings\n",
    "        sampling_params = SamplingParams(temperature=temperature, top_p=top_p, max_tokens=max_tokens)\n",
    "\n",
    "        correct = 0\n",
    "        num_questions = 0\n",
    "\n",
    "        answers = []\n",
    "\n",
    "        # batch inference\n",
    "        for _, batch in tqdm(enumerate(eval_dataloader), desc=\"Eval Inference: \", total=len(eval_dataloader)):\n",
    "            # inputs = tokenizer(batch[\"prompt\"], return_tensors=\"pt\", max_length=256, padding=\"max_length\", truncation=True).to(device)\n",
    "            # outputs = model.generate(**inputs, max_new_tokens=256)\n",
    "            # batch_answers = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            # answers.extend(batch_answers)\n",
    "            prompts = batch[\"prompt\"]\n",
    "            outputs = llm.generate(prompts, sampling_params)\n",
    "            batch_answers = [output.outputs[0].text.strip() for output in outputs]\n",
    "            answers.extend(batch_answers)\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # text parse for exact match\n",
    "        for i, (correct_answer, generated_answer) in tqdm(enumerate(zip(eval_dataset['answer'], answers)), desc=\"Evaluating Exact Match Accuracy: \", total=len(eval_dataset)):\n",
    "            # # Remove the input tokens from the output for transformers inference\n",
    "            # generated_answer = generated_answer[len(eval_dataset['prompt'][i]):]\n",
    "\n",
    "            # Extract final answer\n",
    "            predicted_answer = self.parse_answer(generated_answer)\n",
    "            ground_truth = self.parse_answer(correct_answer)\n",
    "\n",
    "            # Check if the predicted answer matches the ground truth\n",
    "            if ground_truth:\n",
    "                num_questions += 1\n",
    "                if predicted_answer and predicted_answer == ground_truth:\n",
    "                    correct += 1\n",
    "\n",
    "        return correct / num_questions if num_questions > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0484724d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ds = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\", num_proc=4)\n",
    "evaluator = FewShotEvaluator(eval_ds, n_shots=3, device=\"cuda\", batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b934db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-22 12:45:10 [config.py:689] This model supports multiple tasks: {'generate', 'embed', 'classify', 'reward', 'score'}. Defaulting to 'generate'.\n",
      "INFO 04-22 12:45:11 [config.py:1901] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 04-22 12:45:11 [core.py:61] Initializing a V1 LLM engine (v0.8.4) with config: model='Qwen/Qwen2.5-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2025-04-22 12:45:13,718 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-22 12:45:14 [utils.py:2444] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x1553c8d064b0>\n",
      "INFO 04-22 12:45:14 [parallel_state.py:959] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 04-22 12:45:14 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "INFO 04-22 12:45:15 [gpu_model_runner.py:1276] Starting to load model Qwen/Qwen2.5-1.5B-Instruct...\n",
      "INFO 04-22 12:45:15 [topk_topp_sampler.py:44] Currently, FlashInfer top-p & top-k sampling sampler is disabled because FlashInfer>=v0.2.3 is not backward compatible. Falling back to the PyTorch-native implementation of top-p & top-k sampling.\n",
      "INFO 04-22 12:45:15 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "INFO 04-22 12:45:15 [weight_utils.py:315] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14ef63a7944a4a3093c5dd5566ce9e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-22 12:45:16 [loader.py:458] Loading weights took 0.71 seconds\n",
      "INFO 04-22 12:45:16 [gpu_model_runner.py:1291] Model loading took 2.8876 GiB and 1.129856 seconds\n",
      "INFO 04-22 12:45:23 [backends.py:416] Using cache directory: /network/rit/home/ptian_wang_lab_cs/.cache/vllm/torch_compile_cache/d3bcff9216/rank_0_0 for vLLM's torch.compile\n",
      "INFO 04-22 12:45:23 [backends.py:426] Dynamo bytecode transform time: 7.41 s\n",
      "INFO 04-22 12:45:27 [backends.py:132] Cache the graph of shape None for later use\n",
      "INFO 04-22 12:45:47 [backends.py:144] Compiling a graph for general shape takes 23.06 s\n",
      "INFO 04-22 12:45:58 [monitor.py:33] torch.compile takes 30.48 s in total\n",
      "INFO 04-22 12:45:59 [kv_cache_utils.py:634] GPU KV cache size: 2,482,944 tokens\n",
      "INFO 04-22 12:45:59 [kv_cache_utils.py:637] Maximum concurrency for 32,768 tokens per request: 75.77x\n",
      "INFO 04-22 12:46:29 [gpu_model_runner.py:1626] Graph capturing finished in 30 secs, took 1.47 GiB\n",
      "INFO 04-22 12:46:29 [core.py:163] init engine (profile, create kv cache, warmup model) took 73.13 seconds\n",
      "INFO 04-22 12:46:29 [core_client.py:435] Core engine process 0 ready.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49f1e9f6d1ce493aabf3ccfd635bcf18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval Inference:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef9c61e44d74da48de06e1fb6dc58e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                            | 0/128 [00:00<?, ?it/s, e…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16b72226d7394bc699f507e3838892ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                            | 0/128 [00:00<?, ?it/s, e…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b025377602684a7f896fa71c55894f0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                            | 0/128 [00:00<?, ?it/s, e…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40562034260342f4a61a345ab8951a0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                            | 0/128 [00:00<?, ?it/s, e…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be9151484c544a0687de482620768ecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                            | 0/128 [00:00<?, ?it/s, e…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a146b41b3043748c30200f1d67886d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                            | 0/128 [00:00<?, ?it/s, e…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a494891ef546423c8da085e3c6ef8b70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                            | 0/128 [00:00<?, ?it/s, e…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f8589bd219348c4b1c7645211b73791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                            | 0/128 [00:00<?, ?it/s, e…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a9fe7853b0e4947a8ef08ef344db454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                            | 0/128 [00:00<?, ?it/s, e…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ef8073b14f94c018974a031b4514fad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                            | 0/128 [00:00<?, ?it/s, e…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0f6be395e8d432fb27e057714b171ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                             | 0/36 [00:00<?, ?it/s, e…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "386b4497a3ea46e9817bd5b0e919caaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Exact Match Accuracy:   0%|          | 0/1316 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.5851063829787234"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pretrain evaluation\n",
    "qem1 = evaluator.eval(baseline, device=\"cuda\")\n",
    "qem1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81d85fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d81cf39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-24 08:21:33 [config.py:689] This model supports multiple tasks: {'generate', 'classify', 'embed', 'reward', 'score'}. Defaulting to 'generate'.\n",
      "INFO 04-24 08:21:33 [config.py:1901] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 04-24 08:21:36 [core.py:61] Initializing a V1 LLM engine (v0.8.4) with config: model='/network/rit/lab/wang_lab_cs/ptian/output/rft/qwen2.5-1.5b-instruct-grpo-20250423-080507', speculative_config=None, tokenizer='/network/rit/lab/wang_lab_cs/ptian/output/rft/qwen2.5-1.5b-instruct-grpo-20250423-080507', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/network/rit/lab/wang_lab_cs/ptian/output/rft/qwen2.5-1.5b-instruct-grpo-20250423-080507, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 08:21:38,844 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-24 08:21:41 [utils.py:2444] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x1553c93906e0>\n",
      "INFO 04-24 08:21:43 [parallel_state.py:959] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 04-24 08:21:43 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "INFO 04-24 08:21:43 [gpu_model_runner.py:1276] Starting to load model /network/rit/lab/wang_lab_cs/ptian/output/rft/qwen2.5-1.5b-instruct-grpo-20250423-080507...\n",
      "INFO 04-24 08:21:44 [topk_topp_sampler.py:44] Currently, FlashInfer top-p & top-k sampling sampler is disabled because FlashInfer>=v0.2.3 is not backward compatible. Falling back to the PyTorch-native implementation of top-p & top-k sampling.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff2ec458988b4fdc8071706f88afedcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-24 08:21:54 [loader.py:458] Loading weights took 10.62 seconds\n",
      "INFO 04-24 08:21:55 [gpu_model_runner.py:1291] Model loading took 2.8876 GiB and 11.600211 seconds\n",
      "INFO 04-24 08:22:13 [backends.py:416] Using cache directory: /network/rit/home/ptian_wang_lab_cs/.cache/vllm/torch_compile_cache/5bef00c68c/rank_0_0 for vLLM's torch.compile\n",
      "INFO 04-24 08:22:13 [backends.py:426] Dynamo bytecode transform time: 17.81 s\n",
      "INFO 04-24 08:22:17 [backends.py:132] Cache the graph of shape None for later use\n",
      "INFO 04-24 08:22:38 [backends.py:144] Compiling a graph for general shape takes 24.81 s\n",
      "INFO 04-24 08:22:50 [monitor.py:33] torch.compile takes 42.62 s in total\n",
      "INFO 04-24 08:22:52 [kv_cache_utils.py:634] GPU KV cache size: 2,482,944 tokens\n",
      "INFO 04-24 08:22:52 [kv_cache_utils.py:637] Maximum concurrency for 32,768 tokens per request: 75.77x\n",
      "INFO 04-24 08:23:22 [gpu_model_runner.py:1626] Graph capturing finished in 30 secs, took 1.47 GiB\n",
      "INFO 04-24 08:23:22 [core.py:163] init engine (profile, create kv cache, warmup model) took 87.27 seconds\n",
      "INFO 04-24 08:23:22 [core_client.py:435] Core engine process 0 ready.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb52b3eea4746ec95a7d596b3a029b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval Inference:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d7aa9aa4dd04e99a60a5776cdb65846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3df5256d6e416d97e5072f2ee7397d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e56f0f4c090466898560efd972837eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50dbaca19f5747cda6443969d0cfa649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "450dc168160d470bb3b5088ebca10890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81cedf9774604e7fbd41d53a5047c9a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "046785df986c4ec19b0b28e2e1ebf3dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "799418f2e7c749388130117ab9ab7742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f04f3347a0a04a6b8792bee08b15746d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "197e030410a04185b47022d5c37d2ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a402a13bca1e4dda90635a63154bac8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|           | 0/36 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0ecb7976fdb474793f49723a55a0b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Exact Match Accuracy:   0%|          | 0/1316 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.5965045592705167"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finetune evaluation\n",
    "qem2 = evaluator.eval(finetune, device=\"cuda\")\n",
    "qem2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65165700",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
